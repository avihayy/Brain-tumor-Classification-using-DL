{"cells":[{"cell_type":"markdown","metadata":{"id":"YMYMpxuOrL17"},"source":["**Mount your drive in order to run locally with colab**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1029,"status":"ok","timestamp":1609758984927,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"w6WJeYQRNCVb","outputId":"6bfa3508-7a38-4514-848d-6d0d3f5bde3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"G2pyZfPerVen"},"source":["**Move to the project folder path**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":954,"status":"ok","timestamp":1609760647399,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"jmGtrylWNkH1","outputId":"e0e6b314-0154-48df-d98d-31a8b60c5c11"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1q7jvyApRjOeFGFPz4JGAK3Bk72R-pc3l/medical_image_processing_using_DP/project_204452072_203737739\n"]}],"source":["cd \"/content/drive/MyDrive/medical_image_processing_using_DP/project_204452072_203737739/\""]},{"cell_type":"markdown","metadata":{"id":"_jk0R5Hqsgwx"},"source":["**Import all relevance functions and parameters**\n","\n","* img_size - the size of the images and masks after resizing. Bothe of the networks (segmentation and classification) recieves inputs image and mask of this size (fix- should not change).\n","* split_dataset_to_N= split the dataset to 3 parts (sub-folders) in order to handle with the memory constraints of the Colab (fix- should not change).\n","* how_to_separate_dataset -  \"ramdom\" for random approach or \"patient\" for subject approach (can change: \"random\" or \"patient\" but need to stay fix for both networks).\n","* num_classes - number of classes for the 3 sub-classificatio networks, each one of them classify between 2 tumors types (fix- should not chnage).  \n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2829,"status":"ok","timestamp":1609760651082,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"CzchWzYVspR2"},"outputs":[],"source":["from tumor_classification_NN import train_Unet_segmentation_model,train_all_classification_sub_models,test_model,create_classification_model,unet_model\n","img_size = (256,256)\n","split_dataset_to_N = 3\n","how_to_separate_dataset=\"patient\" # can be random or patient\n","num_classes = 2"]},{"cell_type":"markdown","metadata":{"id":"u-S5cLWurx6z"},"source":["**Training part**:"]},{"cell_type":"markdown","metadata":{"id":"BW4XmG7q8M7v"},"source":["*Train segmentation Unet NN*:"]},{"cell_type":"markdown","metadata":{"id":"l2IKgiNO8Z3U"},"source":["*save weights path:*\n","\n","Insert the path where you want to save the weights for the segmentation network, for example:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":946,"status":"ok","timestamp":1609760582408,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"kvkUYK2P8fyX"},"outputs":[],"source":["segmentation_model_save_path =\"saved_classifier_model/example_save_weights_file_unet_model_\" + how_to_separate_dataset + \"_separation.h5\""]},{"cell_type":"markdown","metadata":{"id":"CQWhTAJeC85k"},"source":["*Train segmentation model*:\n","\n","- epochs_segmentation_model - number of epochs (can change).\n","- batch_size_segmentation_model - number of images per betch (can change)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDB9NVDO8YXK"},"outputs":[],"source":["epochs_segmentation_model = 100\n","batch_size_segmentation_model = 32\n","\n","train_Unet_segmentation_model(segmentation_model_save_path,how_to_separate_dataset,epochs_segmentation_model,batch_size_segmentation_model,img_size)"]},{"cell_type":"markdown","metadata":{"id":"EGE5RND1zoOj"},"source":["*Train clasification model (contain 3 sub models)*:"]},{"cell_type":"markdown","metadata":{"id":"0-C-YKPLJKT9"},"source":["save weights path:\n","\n","Insert the path where you want to save the weights for the segmentation network, for example:"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":997,"status":"ok","timestamp":1609760661548,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"VB0h1R4cJPQf"},"outputs":[],"source":["classification_model_save_path = \"saved_classifier_model/example_save_weights_file_clasification_model_\"+how_to_separate_dataset+\"_separation_\""]},{"cell_type":"markdown","metadata":{"id":"md1aDZmPJZ90"},"source":["*Train classification model*:\n","\n","- epochs_segmentation_model - number of epochs (can change).\n","- batch_size_segmentation_model - number of images per betch (can change).\n","- with_mask - train the classification networks with (True) or without (False) the masks.\n","- itr - number of iterations in the train which means the number of epochs is epochs_classification_model x itr (can change)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPflTwxbz3Xz"},"outputs":[],"source":["epochs_classification_model = 30\n","batch_size_classification_model = 32\n","with_mask = True\n","\n","itr = split_dataset_to_N * 5 #(total 5*30=150 epochs)\n","\n","train_all_classification_sub_models (epochs_classification_model, batch_size_classification_model, img_size,num_classes, with_mask, classification_model_save_path, split_dataset_to_N, itr, how_to_separate_dataset)"]},{"cell_type":"markdown","metadata":{"id":"kzUh4-zQ3nst"},"source":["**Testing part- test entire Model - random approach:**"]},{"cell_type":"markdown","metadata":{"id":"62Mke4Df-X70"},"source":["*Set parameters and load weights for all the networks and sub-networks* (segmentation and classification).\n","* use_pred_mask- \n","\n","  - True for using the masks created by the the segmentation network. \n","\n","  - False- for using the original masks (Groung Truth masks)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAYXv10q9rMg"},"outputs":[],"source":["use_pred_mask = True\n","\n","segmentation_model_load_path=\"saved_classifier_model/unet_segmentation_model_random_separation.h5\"\n","classification_model_load_path=\"saved_classifier_model/Model_with_pred_mask_random_separation_\"\n","\n","model_label_1_2 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_1_3 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_2_3 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_1_2.load_weights(classification_model_load_path+\"label_1_2.h5\")\n","model_label_1_3.load_weights(classification_model_load_path+\"label_1_3.h5\")\n","model_label_2_3.load_weights(classification_model_load_path+\"label_2_3.h5\")\n","segmentation_model = unet_model(img_size + (1,), use_attention=True)\n","segmentation_model.load_weights(segmentation_model_load_path)\n","\n","test_model(segmentation_model, model_label_1_2, model_label_1_3, model_label_2_3, img_size, with_mask,\"random\",use_pred_mask)"]},{"cell_type":"markdown","metadata":{"id":"cLPIyrAY_aRe"},"source":["**Testing part- test entire Model - subject approach:**"]},{"cell_type":"markdown","metadata":{"id":"LNXdq3adMtyu"},"source":["*load model weights*:\n","\n","Insert the path where the all wheights are saved sits in your google drive. \n","\n","for example, in our drive this folder sits at the following path:"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":939,"status":"ok","timestamp":1609761069614,"user":{"displayName":"Ronly Botnaro","photoUrl":"","userId":"12713843865343791966"},"user_tz":-120},"id":"M9RFcuus_gO7"},"outputs":[],"source":["segmentation_model_load_path=\"saved_classifier_model/unet_segmentation_model_patient_separation.h5\"\n","classification_model_load_path=\"saved_classifier_model/Model_with_pred_mask_patient_separation_\""]},{"cell_type":"markdown","metadata":{"id":"Dh-uKzm5_lbg"},"source":["Set parameters and load weights of all the networks and sub-networks (segmentation and classification)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"L_gn_pYF_y43"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1q7jvyApRjOeFGFPz4JGAK3Bk72R-pc3l/medical_image_processing_using_DP/project_204452072_203737739/prepare_dataset.py:229: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  PID = (data.get('PID').value).reshape(-1)\n","/content/drive/.shortcut-targets-by-id/1q7jvyApRjOeFGFPz4JGAK3Bk72R-pc3l/medical_image_processing_using_DP/project_204452072_203737739/prepare_dataset.py:234: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  img_data=image.value\n","/content/drive/.shortcut-targets-by-id/1q7jvyApRjOeFGFPz4JGAK3Bk72R-pc3l/medical_image_processing_using_DP/project_204452072_203737739/prepare_dataset.py:235: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  mask_data = mask.value\n","/content/drive/.shortcut-targets-by-id/1q7jvyApRjOeFGFPz4JGAK3Bk72R-pc3l/medical_image_processing_using_DP/project_204452072_203737739/prepare_dataset.py:236: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  label=lab_temp.value[0][0]\n"]},{"name":"stdout","output_type":"stream","text":["##########################################################################\n","patient id which tested(total 38):['88510' '97374' '97461' '97481' '97599' '97607' '97737' '97793' '97875'\n"," '97890' '97943' '98241' '98472' '98871' '98992' '98995' '99017' '99089'\n"," '99162' '99223' '99288' '99308' '99815' 'MR051461' 'MR051586' 'MR051644'\n"," 'MR051644B' 'MR051644C' 'MR051651B' 'MR051796' 'MR051796B' 'MR052550'\n"," 'MR052585B' 'MR053110B' 'MR053110E' 'MR053241C' 'MR054683' 'MR056144']\n","##########################################################################\n","%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%results%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n","Number of tested images=545\n","_____________________________________________________________________________________________________________________________________________\n","|                          |     Class 1 (Meningioma)   |        Class 2 (Glioma)        |   Class 3 (Pituitary)    |       recall         |\n","|   Class 1 (Meningioma)   |           112              |             7                  |        3                 |       91.8%          |\n","|   Class 2 (Glioma)       |             1              |           261                  |        1                 |       99.2%          |\n","|   Class 3 (Pituitary)    |             2              |             1                  |      157                 |       98.1%          |\n","|   precision              |           97.4%            |           97.0%                |      97.5%               |       97.2%          |\n","_____________________________________________________________________________________________________________________________________________\n","average precision = 97.3%\n","average recall = 96.4%\n","F1 score = 96.8%\n","total accuracy = 97.2%\n","total prediction time= 34.867sec, time per image= 0.064sec\n","%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"]}],"source":["use_pred_mask = True\n","model_label_1_2 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_1_3 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_2_3 = create_classification_model(img_size, num_classes, with_mask)\n","model_label_1_2.load_weights(classification_model_load_path+\"label_1_2.h5\")\n","model_label_1_3.load_weights(classification_model_load_path+\"label_1_3.h5\")\n","model_label_2_3.load_weights(classification_model_load_path+\"label_2_3.h5\")\n","segmentation_model = unet_model(img_size + (1,), use_attention=True)\n","segmentation_model.load_weights(segmentation_model_load_path)\n","\n","test_model(segmentation_model, model_label_1_2, model_label_1_3, model_label_2_3, img_size, with_mask,\"patient\",use_pred_mask)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"project_run.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}